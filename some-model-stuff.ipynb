{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#tell about what is Sequestial \n",
    "#Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(784,)),\n",
    "    Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Another way to create a sequential NN \n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets learn (layer) activation functions in keras\n",
    "\n",
    "https://keras.io/api/layers/activations/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relu function\n",
    "\n",
    "#tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "\n",
    "Applies the rectified linear unit activation function.\n",
    "\n",
    "With default values, this returns the standard ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
    "\n",
    "Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)\n",
    ">>> tf.keras.activations.relu(foo).numpy()\n",
    "array([ 0.,  0.,  0.,  5., 10.], dtype=float32)\n",
    "\n",
    "\n",
    ">>> tf.keras.activations.relu(foo, alpha=0.5).numpy()\n",
    "array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)\n",
    "\n",
    "\n",
    ">>> tf.keras.activations.relu(foo, max_value=5).numpy()\n",
    "array([0., 0., 0., 5., 5.], dtype=float32)\n",
    "\n",
    "\n",
    ">>> tf.keras.activations.relu(foo, threshold=5).numpy()\n",
    "array([-0., -0.,  0.,  0., 10.], dtype=float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid\n",
    "tf.keras.activations.sigmoid(x)\n",
    "\n",
    "Sigmoid activation function, sigmoid(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "Applies the sigmoid activation function. For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1.\n",
    "\n",
    "Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero. \n",
    "* The sigmoid function always returns a value between 0 and 1\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n",
    ">>> b = tf.keras.activations.sigmoid(a)\n",
    ">>> b.numpy()\n",
    "array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,\n",
    "         1.0000000e+00], dtype=float32)\n",
    "\n",
    "Arguments\n",
    "\n",
    "    x: Input tensor.\n",
    "\n",
    "Returns\n",
    "\n",
    "Tensor with the sigmoid activation: 1 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax function\n",
    "\n",
    "tf.keras.activations.softmax(x, axis=-1)\n",
    "\n",
    "Softmax converts a real vector to a vector of categorical probabilities.\n",
    "\n",
    "The elements of the output vector are in range (0, 1) and sum to 1.\n",
    "\n",
    "Each vector is handled independently. The axis argument sets which axis of the input the function is applied along.\n",
    "\n",
    "Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.\n",
    "\n",
    "The softmax of each vector x is computed as exp(x) / tf.reduce_sum(exp(x)).\n",
    "\n",
    "The input values in are the log-odds of the resulting probability.\n",
    "\n",
    "Arguments\n",
    "\n",
    "    x : Input tensor.\n",
    "    axis: Integer, axis along which the softmax normalization is applied.\n",
    "\n",
    "Returns\n",
    "\n",
    "Tensor, output of softmax transformation (all values are non-negative and sum to 1).\n",
    "\n",
    "Raises\n",
    "\n",
    "    ValueError: In case dim(x) == 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh function\n",
    "\n",
    "tf.keras.activations.tanh(x)\n",
    "\n",
    "Hyperbolic tangent activation function.\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    ">>> b = tf.keras.activations.tanh(a)\n",
    ">>> b.numpy()\n",
    "array([-0.9950547, -0.7615942,  0.,  0.7615942,  0.9950547], dtype=float32)\n",
    "\n",
    "Arguments\n",
    "\n",
    "    x: Input tensor.\n",
    "\n",
    "Returns\n",
    "\n",
    "Tensor of same shape and dtype of input x, with tanh activation: tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exponential function\n",
    "\n",
    "tf.keras.activations.exponential(x)\n",
    "\n",
    "Exponential activation function.\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    ">>> b = tf.keras.activations.exponential(a)\n",
    ">>> b.numpy()\n",
    "array([0.04978707,  0.36787945,  1.,  2.7182817 , 20.085537], dtype=float32)\n",
    "\n",
    "Arguments\n",
    "\n",
    "    x: Input tensor.\n",
    "\n",
    "Returns\n",
    "\n",
    "Tensor with exponential activation: exp(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some more.\n",
    "\n",
    "Refer: https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Next\n",
    "\n",
    "\n",
    "* Dense(32, activation=\"relu\", input_shape=(784,)),\n",
    "input = (784,) dimn, oupt - 32 dimn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've done with our model creation and next is compilation of model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compilation step, three arguments are required/helpful\n",
    " * An optimizer - Its is lesarning technique\n",
    " * A loss function - objective to minimize loss\n",
    " * A list of matrix - for accuracy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a multi-class classification problem \n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "#For a binary classification problems\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#For a mean squared error regression problem \n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='mae')\n",
    "\n",
    "#For custom matrics\n",
    "import keras.backend as K\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binarycrossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets learn Optimizers \n",
    "An optimizer is one of the two arguments required for compiling a Keras model:\n",
    "    \n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "# SGD\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0., decay=0., nesterov=False)\n",
    "Arguments:\n",
    "\n",
    "lr: float >= 0. Learning rate.\n",
    "momentum: float >= 0. Parameter updates momentum.\n",
    "decay: float >= 0. Learning rate decay over each update.\n",
    "nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",
    ")\n",
    "\n",
    "Gradient descent (with momentum) optimizer.\n",
    "\n",
    "Update rule for parameter w with gradient g when momentum is 0:\n",
    "\n",
    "w = w - learning_rate * g\n",
    "\n",
    "Update rule when momentum is larger than 0:\n",
    "\n",
    "velocity = momentum * velocity - learning_rate * g\n",
    "\n",
    "w = w + velocity\n",
    "\n",
    "When nesterov=False, this rule becomes:\n",
    "\n",
    "velocity = momentum * velocity - learning_rate * g\n",
    "\n",
    "w = w + momentum * velocity - learning_rate * g\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad\n",
    "keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "lr: float >= 0. Learning rate.\n",
    "epsilon: float >= 0.\n",
    "\n",
    "# RMSprop\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RMSprop class\n",
    "tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    name=\"RMSprop\",\n",
    "    **kwargs\n",
    ")\n",
    "Optimizer that implements the RMSprop algorithm.\n",
    "\n",
    "The gist of RMSprop is to:\n",
    "\n",
    "Maintain a moving (discounted) average of the square of gradients\n",
    "Divide the gradient by the root of this average\n",
    "This implementation of RMSprop uses plain momentum, not Nesterov momentum.\n",
    "\n",
    "The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "Adam class\n",
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name=\"Adam\",\n",
    "    **kwargs\n",
    ")\n",
    "Optimizer that implements the Adam algorithm.\n",
    "\n",
    "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "\n",
    "According to Kingma et al., 2014, the method is \"computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more refer here: https://keras.io/api/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets dig on loss "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Keras, loss functions are passed during the compile stage as shown below. \n",
    "\n",
    "In this example, we’re defining the loss function by creating an instance of the loss class. Using the class is advantageous because you can pass some additional parameters. \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_function, optimizer='adam')\n",
    "If you want to use a loss function that is built into Keras without specifying any parameters you can just use the string alias as shown below:\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "You might be wondering, how does one decide on which loss function to use?\n",
    "\n",
    "There are various loss functions available in Keras. Other times you might have to implement your own custom loss functions. \n",
    "\n",
    "Let’s dive into all those scenarios.\n",
    "\n",
    "Which Loss functions are available in Keras?\n",
    "\n",
    "##Binary Classification\n",
    "\n",
    "Binary classification loss function comes into play when solving a problem involving just two classes. For example, when predicting fraud in credit card transactions, a transaction is either fraudulent or not. \n",
    "\n",
    "##Binary Cross Entropy\n",
    "\n",
    "The Binary Cross entropy will calculate the cross-entropy loss between the predicted classes and the true classes. By default, the sum_over_batch_size reduction is used. This means that the loss will return the average of the per-sample losses in the batch.\n",
    "\n",
    "y_true = [[0., 1.], [0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6],[0.6, 0.4],[0.8, 0.2]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
    "bce(y_true, y_pred).numpy()\n",
    "The sum reduction means that the loss function will return the sum of the per-sample losses in the batch.\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction=’sum’)\n",
    "bce(y_true, y_pred).numpy()\n",
    "Using the reduction as none returns the full array of the per-sample losses.\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction=’none’)\n",
    "bce(y_true, y_pred).numpy()\n",
    "array([0.9162905 , 0.5919184 , 0.79465103, 1.0549198 ], dtype=float32)\n",
    "In binary classification, the activation function used is the sigmoid activation function. It constrains the output to a number between 0 and 1. \n",
    "\n",
    "##Multiclass classification\n",
    "Problems involving the prediction of more than one class use different loss functions. In this section we’ll look at a couple:\n",
    "\n",
    "##Categorical Crossentropy\n",
    "The CategoricalCrossentropy also computes the cross-entropy loss between the true classes and predicted classes. The labels are given in an one_hot format. \n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()cce(y_true, y_pred).numpy()\n",
    "Sparse Categorical Crossentropy\n",
    "If you have two or more classes and  the labels are integers, the SparseCategoricalCrossentropy should be used. \n",
    "\n",
    "y_true = [0, 1,2]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "scce(y_true, y_pred).numpy()\n",
    "The Poison Loss\n",
    "You can also use the Poisson class to compute the poison loss. It’s a great choice if your dataset comes from a Poisson distribution for example the number of calls a call center receives per hour. \n",
    "\n",
    "y_true = [[0.1, 1.,0.8], [0.1, 0.9,0.1],[0.2, 0.7,0.1],[0.3, 0.1,0.6]]\n",
    "y_pred = [[0.6, 0.2,0.2], [0.2, 0.6,0.2],[0.7, 0.1,0.2],[0.8, 0.1,0.1]]\n",
    "p = tf.keras.losses.Poisson()\n",
    "p(y_true, y_pred).numpy()\n",
    "Kullback-Leibler Divergence Loss\n",
    "The relative entropy can be computed using the KLDivergence class. According to the official docs at PyTorch:\n",
    "\n",
    "KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. \n",
    "\n",
    "y_true = [[0.1, 1.,0.8], [0.1, 0.9,0.1],[0.2, 0.7,0.1],[0.3, 0.1,0.6]]\n",
    "y_pred = [[0.6, 0.2,0.2], [0.2, 0.6,0.2],[0.7, 0.1,0.2],[0.8, 0.1,0.1]]\n",
    "kl = tf.keras.losses.KLDivergence()\n",
    "kl(y_true, y_pred).numpy()\n",
    "In a multi-class problem, the activation function used is the softmax function.\n",
    "\n",
    "##Object Detection\n",
    "The Focal Loss\n",
    "In classification problems involving imbalanced data and object detection problems, you can use the Focal Loss. The loss introduces an adjustment to the cross-entropy criterion. \n",
    "\n",
    "\n",
    "It is done by altering its shape in a way that the loss allocated to well-classified examples is down-weighted. This ensures that the model is able to learn equally from minority and majority classes.\n",
    "\n",
    "The cross-entropy loss is scaled by scaling the factors decaying at zero as the confidence in the correct class increases. The factor of scaling down weights the contribution of unchallenging samples at training time and focuses on the challenging ones.\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "y_true = [[0.97], [0.91], [0.03]]\n",
    "y_pred = [[1.0], [1.0], [0.0]]\n",
    "sfc = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "sfc(y_true, y_pred).numpy()\n",
    "array([0.00010971, 0.00329749, 0.00030611], dtype=float32)\n",
    "Generalized Intersection over Union\n",
    "The Generalized Intersection over Union loss from the TensorFlow add on can also be used. The Intersection over Union (IoU) is a very common metric in object detection problems. IoU is however not very efficient in problems involving non-overlapping bounding boxes. \n",
    "\n",
    "The Generalized Intersection over Union was introduced to address this challenge that IoU is facing. It ensures that generalization is achieved by maintaining the scale-invariant property of IoU, encoding the shape properties of the compared objects into the region property, and making sure that there is a strong correlation with IoU in the event of overlapping objects. \n",
    "\n",
    "gl = tfa.losses.GIoULoss()\n",
    "boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n",
    "boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n",
    "loss = gl(boxes1, boxes2)\n",
    "Regression\n",
    "In regression problems, you have to calculate the differences between the predicted values and the true values but as always there are many ways to do it.\n",
    "\n",
    "##Mean Squared Error\n",
    "The MeanSquaredError class can be used to compute the mean square of errors between the predictions and the true values. \n",
    "\n",
    "y_true = [12, 20, 29., 60.]\n",
    "y_pred = [14., 18., 27., 55.]\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()\n",
    "Use Mean Squared Error when you desire to have large errors penalized more than smaller ones. \n",
    "\n",
    "##Mean Absolute Percentage Error\n",
    "The mean absolute percentage error is computed using the function below.\n",
    "\n",
    "\n",
    "It is calculated as shown below.\n",
    "\n",
    "y_true = [12, 20, 29., 60.]\n",
    "y_pred = [14., 18., 27., 55.]\n",
    "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "mape(y_true, y_pred).numpy()\n",
    "Consider using this loss when you want a loss that you can explain intuitively. People understand percentages easily. The loss is also robust to outliers. \n",
    "\n",
    "##Mean Squared Logarithmic Error\n",
    "The mean squared logarithmic error can be computed using the formula below:\n",
    "\n",
    "\n",
    "Here’s an implementation of the same:\n",
    "\n",
    "y_true = [12, 20, 29., 60.]\n",
    "y_pred = [14., 18., 27., 55.]\n",
    "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "msle(y_true, y_pred).numpy()\n",
    "Mean Squared Logarithmic Error penalizes underestimates more than it does overestimates. It’s a great choice when you prefer not to penalize large errors, it is, therefore, robust to outliers. \n",
    "\n",
    "##Cosine Similarity Loss\n",
    "If your interest is in computing the cosine similarity between the true and predicted values, you’d use the CosineSimilarity class. It is computed as:\n",
    "\n",
    "\n",
    "The result is a negative number between -1 and 0. 0 indicates orthogonality while values close to -1 show that there is great similarity. \n",
    "\n",
    "y_true = [[12, 20], [29., 60.]]\n",
    "y_pred = [[14., 18.], [27., 55.]]\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "cosine_loss(y_true, y_pred).numpy()\n",
    "LogCosh Loss\n",
    "The LogCosh class computes the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "\n",
    "Here’s its implementation as a stand-alone function. \n",
    "\n",
    "y_true = [[12, 20], [29., 60.]]\n",
    "y_pred = [[14., 18.], [27., 55.]]\n",
    "l = tf.keras.losses.LogCosh()\n",
    "l(y_true, y_pred).numpy()\n",
    "LogCosh Loss works like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. — TensorFlow Docs\n",
    "\n",
    "##Huber loss\n",
    "For regression problems that are less sensitive to outliers, the Huber loss is used. \n",
    "\n",
    "y_true = [12, 20, 29., 60.]\n",
    "y_pred = [14., 18., 27., 55.]\n",
    "h = tf.keras.losses.Huber()\n",
    "h(y_true, y_pred).numpy()\n",
    "Learning Embeddings\n",
    "Triplet Loss\n",
    "You can also compute the triplet loss with semi-hard negative mining via TensorFlow addons. The loss encourages the positive distances between pairs of embeddings with the same labels to be less than the minimum negative distance. \n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "model.compile(optimizer=’adam’,\n",
    "loss=tfa.losses.TripletSemiHardLoss(),\n",
    "metrics=[‘accuracy’])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32,activation='relu', input_dim=100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "#Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000,100))\n",
    "labels = np.random.randint(2, size=(1000,1))\n",
    "#model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 - 0s - loss: 0.7072 - accuracy: 0.4730\n",
      "Epoch 2/10\n",
      "32/32 - 0s - loss: 0.6953 - accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "32/32 - 0s - loss: 0.6887 - accuracy: 0.5510\n",
      "Epoch 4/10\n",
      "32/32 - 0s - loss: 0.6858 - accuracy: 0.5340\n",
      "Epoch 5/10\n",
      "32/32 - 0s - loss: 0.6823 - accuracy: 0.5530\n",
      "Epoch 6/10\n",
      "32/32 - 0s - loss: 0.6791 - accuracy: 0.5710\n",
      "Epoch 7/10\n",
      "32/32 - 0s - loss: 0.6764 - accuracy: 0.5720\n",
      "Epoch 8/10\n",
      "32/32 - 0s - loss: 0.6744 - accuracy: 0.5830\n",
      "Epoch 9/10\n",
      "32/32 - 0s - loss: 0.6701 - accuracy: 0.5830\n",
      "Epoch 10/10\n",
      "32/32 - 0s - loss: 0.6672 - accuracy: 0.6140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13def32b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        data,\n",
    "        labels,\n",
    "        batch_size=32,\n",
    "        epochs=10, verbose=2,\n",
    "        callbacks=None,\n",
    "        validation_data=None,\n",
    "        shuffle = True,\n",
    "        class_weight=None,\n",
    "        sample_weight=None,\n",
    "        initial_epoch=0)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6667970418930054, 0.6153100728988647]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Runs a single gradient update on a single batch of data.\n",
    "model.train_on_batch(\n",
    "                    data[:32],\n",
    "                    labels[:32],\n",
    "                    class_weight=None,\n",
    "                    sample_weight=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen():\n",
    "    for datus, label in zip(data, labels):\n",
    "        yield datus[None,:], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6788 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13d709100>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train on generator \n",
    "model.fit_generator(\n",
    "                data_gen(),\n",
    "                steps_per_epoch=5, #900\n",
    "                epochs=1,\n",
    "                verbose=1,\n",
    "                callbacks=None,\n",
    "                validation_data=None, #this can be a generator or a dataset\n",
    "                validation_steps=None,\n",
    "                class_weight=None,\n",
    "                #max_q_size=10,\n",
    "                workers=1,\n",
    "                #pickle_safe=False,\n",
    "                initial_epoch=0)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "* evaluate test on batch\n",
    "* predict/predict_classes/predict_proba(only predict for generator etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 544us/step - loss: 0.6638 - accuracy: 0.6010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6638249754905701, 0.6010000109672546]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(\n",
    "            data,\n",
    "            labels,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 373us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.58177596],\n",
       "       [0.51481676],\n",
       "       [0.48363435],\n",
       "       [0.37324744],\n",
       "       [0.5909763 ],\n",
       "       [0.5085412 ],\n",
       "       [0.5756633 ],\n",
       "       [0.5098746 ],\n",
       "       [0.43131375],\n",
       "       [0.58133817],\n",
       "       [0.5892435 ],\n",
       "       [0.54077   ],\n",
       "       [0.5013192 ],\n",
       "       [0.5410947 ],\n",
       "       [0.62009776],\n",
       "       [0.34385467],\n",
       "       [0.51875365],\n",
       "       [0.5697975 ],\n",
       "       [0.47216827],\n",
       "       [0.6092778 ],\n",
       "       [0.41083294],\n",
       "       [0.58048546],\n",
       "       [0.5323728 ],\n",
       "       [0.5453252 ],\n",
       "       [0.48651725],\n",
       "       [0.55320704],\n",
       "       [0.6096177 ],\n",
       "       [0.5063046 ],\n",
       "       [0.5143441 ],\n",
       "       [0.51943004],\n",
       "       [0.5602287 ],\n",
       "       [0.5128446 ],\n",
       "       [0.56864613],\n",
       "       [0.5354724 ],\n",
       "       [0.5388147 ],\n",
       "       [0.54790336],\n",
       "       [0.53065485],\n",
       "       [0.44630828],\n",
       "       [0.5328061 ],\n",
       "       [0.65760213],\n",
       "       [0.46917966],\n",
       "       [0.45668143],\n",
       "       [0.5905872 ],\n",
       "       [0.45381984],\n",
       "       [0.5425263 ],\n",
       "       [0.60115963],\n",
       "       [0.4705356 ],\n",
       "       [0.6448649 ],\n",
       "       [0.5805385 ],\n",
       "       [0.49101174],\n",
       "       [0.604499  ],\n",
       "       [0.48229808],\n",
       "       [0.4947881 ],\n",
       "       [0.61006194],\n",
       "       [0.6359745 ],\n",
       "       [0.6629744 ],\n",
       "       [0.62611383],\n",
       "       [0.4822327 ],\n",
       "       [0.48648828],\n",
       "       [0.5855813 ],\n",
       "       [0.61858004],\n",
       "       [0.49099502],\n",
       "       [0.44074893],\n",
       "       [0.5807049 ],\n",
       "       [0.5826957 ],\n",
       "       [0.5644451 ],\n",
       "       [0.37011513],\n",
       "       [0.5609652 ],\n",
       "       [0.5569893 ],\n",
       "       [0.56548595],\n",
       "       [0.557604  ],\n",
       "       [0.5371765 ],\n",
       "       [0.59912646],\n",
       "       [0.5704123 ],\n",
       "       [0.49818105],\n",
       "       [0.63446045],\n",
       "       [0.42390922],\n",
       "       [0.5950625 ],\n",
       "       [0.59411985],\n",
       "       [0.6404207 ],\n",
       "       [0.55670804],\n",
       "       [0.55618924],\n",
       "       [0.59857   ],\n",
       "       [0.6043004 ],\n",
       "       [0.5565191 ],\n",
       "       [0.5976366 ],\n",
       "       [0.5248617 ],\n",
       "       [0.5140953 ],\n",
       "       [0.5356883 ],\n",
       "       [0.59041286],\n",
       "       [0.57305956],\n",
       "       [0.5530154 ],\n",
       "       [0.5540613 ],\n",
       "       [0.60329354],\n",
       "       [0.5164435 ],\n",
       "       [0.5365882 ],\n",
       "       [0.57955545],\n",
       "       [0.4717487 ],\n",
       "       [0.5814121 ],\n",
       "       [0.534452  ],\n",
       "       [0.5613296 ],\n",
       "       [0.5201451 ],\n",
       "       [0.56531155],\n",
       "       [0.5396855 ],\n",
       "       [0.45899957],\n",
       "       [0.49455723],\n",
       "       [0.5343444 ],\n",
       "       [0.4909596 ],\n",
       "       [0.4867859 ],\n",
       "       [0.5226418 ],\n",
       "       [0.5404243 ],\n",
       "       [0.5524246 ],\n",
       "       [0.43250236],\n",
       "       [0.50478125],\n",
       "       [0.5375989 ],\n",
       "       [0.5398732 ],\n",
       "       [0.6630514 ],\n",
       "       [0.57632977],\n",
       "       [0.49661553],\n",
       "       [0.5053761 ],\n",
       "       [0.44295838],\n",
       "       [0.5545483 ],\n",
       "       [0.6353956 ],\n",
       "       [0.5358864 ],\n",
       "       [0.5833692 ],\n",
       "       [0.6797298 ],\n",
       "       [0.42872292],\n",
       "       [0.51514864],\n",
       "       [0.57596046],\n",
       "       [0.5506141 ],\n",
       "       [0.56543565],\n",
       "       [0.503982  ],\n",
       "       [0.50474745],\n",
       "       [0.52629536],\n",
       "       [0.42236412],\n",
       "       [0.46824867],\n",
       "       [0.52257746],\n",
       "       [0.56195724],\n",
       "       [0.4597649 ],\n",
       "       [0.56791365],\n",
       "       [0.4607302 ],\n",
       "       [0.62795705],\n",
       "       [0.5715778 ],\n",
       "       [0.53286666],\n",
       "       [0.529911  ],\n",
       "       [0.51806897],\n",
       "       [0.5414875 ],\n",
       "       [0.56459546],\n",
       "       [0.42341202],\n",
       "       [0.5905674 ],\n",
       "       [0.5442033 ],\n",
       "       [0.53160554],\n",
       "       [0.5723476 ],\n",
       "       [0.48176584],\n",
       "       [0.6103639 ],\n",
       "       [0.5138205 ],\n",
       "       [0.6138399 ],\n",
       "       [0.5771618 ],\n",
       "       [0.5275352 ],\n",
       "       [0.5159543 ],\n",
       "       [0.6319519 ],\n",
       "       [0.549439  ],\n",
       "       [0.5761145 ],\n",
       "       [0.42865372],\n",
       "       [0.2963707 ],\n",
       "       [0.4671441 ],\n",
       "       [0.5639633 ],\n",
       "       [0.45143533],\n",
       "       [0.4854626 ],\n",
       "       [0.4555644 ],\n",
       "       [0.49173385],\n",
       "       [0.55257595],\n",
       "       [0.6043271 ],\n",
       "       [0.60038567],\n",
       "       [0.6033316 ],\n",
       "       [0.52967083],\n",
       "       [0.651252  ],\n",
       "       [0.5220652 ],\n",
       "       [0.52825046],\n",
       "       [0.4749895 ],\n",
       "       [0.50582707],\n",
       "       [0.5101579 ],\n",
       "       [0.5283649 ],\n",
       "       [0.49244675],\n",
       "       [0.54641134],\n",
       "       [0.61158615],\n",
       "       [0.48008758],\n",
       "       [0.44878078],\n",
       "       [0.5303739 ],\n",
       "       [0.44558448],\n",
       "       [0.6981847 ],\n",
       "       [0.4773438 ],\n",
       "       [0.46339783],\n",
       "       [0.47132185],\n",
       "       [0.5691614 ],\n",
       "       [0.4920636 ],\n",
       "       [0.53800195],\n",
       "       [0.5641241 ],\n",
       "       [0.5183179 ],\n",
       "       [0.5719949 ],\n",
       "       [0.5457534 ],\n",
       "       [0.6215098 ],\n",
       "       [0.54857165],\n",
       "       [0.5396625 ],\n",
       "       [0.55748767],\n",
       "       [0.52246034],\n",
       "       [0.66210085],\n",
       "       [0.63866097],\n",
       "       [0.47768265],\n",
       "       [0.57954276],\n",
       "       [0.58833146],\n",
       "       [0.5379796 ],\n",
       "       [0.5024652 ],\n",
       "       [0.47586742],\n",
       "       [0.46542624],\n",
       "       [0.51874405],\n",
       "       [0.62522334],\n",
       "       [0.56035095],\n",
       "       [0.3777146 ],\n",
       "       [0.5646743 ],\n",
       "       [0.6112769 ],\n",
       "       [0.4531964 ],\n",
       "       [0.4982675 ],\n",
       "       [0.5277441 ],\n",
       "       [0.5864322 ],\n",
       "       [0.52577597],\n",
       "       [0.5386518 ],\n",
       "       [0.51091284],\n",
       "       [0.4926977 ],\n",
       "       [0.46560982],\n",
       "       [0.62024593],\n",
       "       [0.59142625],\n",
       "       [0.5183876 ],\n",
       "       [0.57075745],\n",
       "       [0.54012966],\n",
       "       [0.55466896],\n",
       "       [0.605686  ],\n",
       "       [0.5136309 ],\n",
       "       [0.5631776 ],\n",
       "       [0.6795104 ],\n",
       "       [0.48870328],\n",
       "       [0.5163    ],\n",
       "       [0.49844962],\n",
       "       [0.5902507 ],\n",
       "       [0.5404836 ],\n",
       "       [0.62408644],\n",
       "       [0.4005914 ],\n",
       "       [0.46308422],\n",
       "       [0.59380287],\n",
       "       [0.4356421 ],\n",
       "       [0.5241218 ],\n",
       "       [0.5203286 ],\n",
       "       [0.54666615],\n",
       "       [0.50199443],\n",
       "       [0.54593974],\n",
       "       [0.5621558 ],\n",
       "       [0.515104  ],\n",
       "       [0.44401678],\n",
       "       [0.5033203 ],\n",
       "       [0.5629863 ],\n",
       "       [0.4378238 ],\n",
       "       [0.3930524 ],\n",
       "       [0.47931904],\n",
       "       [0.55965513],\n",
       "       [0.5597145 ],\n",
       "       [0.5273316 ],\n",
       "       [0.5123346 ],\n",
       "       [0.5956402 ],\n",
       "       [0.5634515 ],\n",
       "       [0.44600934],\n",
       "       [0.64536667],\n",
       "       [0.5500321 ],\n",
       "       [0.49892956],\n",
       "       [0.6558354 ],\n",
       "       [0.5087409 ],\n",
       "       [0.561794  ],\n",
       "       [0.5413153 ],\n",
       "       [0.5303111 ],\n",
       "       [0.54642045],\n",
       "       [0.56814176],\n",
       "       [0.4971294 ],\n",
       "       [0.63997084],\n",
       "       [0.58798325],\n",
       "       [0.5236035 ],\n",
       "       [0.52853596],\n",
       "       [0.558397  ],\n",
       "       [0.5316837 ],\n",
       "       [0.5090827 ],\n",
       "       [0.5265963 ],\n",
       "       [0.4307574 ],\n",
       "       [0.53356546],\n",
       "       [0.63312227],\n",
       "       [0.48293036],\n",
       "       [0.57629013],\n",
       "       [0.55753785],\n",
       "       [0.5965842 ],\n",
       "       [0.45748514],\n",
       "       [0.5279471 ],\n",
       "       [0.6353927 ],\n",
       "       [0.6709193 ],\n",
       "       [0.5581989 ],\n",
       "       [0.6300532 ],\n",
       "       [0.5797044 ],\n",
       "       [0.5196977 ],\n",
       "       [0.55459887],\n",
       "       [0.49245209],\n",
       "       [0.5504899 ],\n",
       "       [0.5054472 ],\n",
       "       [0.50395614],\n",
       "       [0.50078934],\n",
       "       [0.55569804],\n",
       "       [0.45976275],\n",
       "       [0.5187669 ],\n",
       "       [0.5249917 ],\n",
       "       [0.66540045],\n",
       "       [0.6396723 ],\n",
       "       [0.475132  ],\n",
       "       [0.5727713 ],\n",
       "       [0.55391085],\n",
       "       [0.56901026],\n",
       "       [0.48504874],\n",
       "       [0.6461092 ],\n",
       "       [0.6623888 ],\n",
       "       [0.55323166],\n",
       "       [0.53539205],\n",
       "       [0.62378883],\n",
       "       [0.5527836 ],\n",
       "       [0.5138852 ],\n",
       "       [0.6103897 ],\n",
       "       [0.5860998 ],\n",
       "       [0.50941384],\n",
       "       [0.56736475],\n",
       "       [0.5443313 ],\n",
       "       [0.5338296 ],\n",
       "       [0.57146394],\n",
       "       [0.5055846 ],\n",
       "       [0.5293094 ],\n",
       "       [0.57135564],\n",
       "       [0.55058515],\n",
       "       [0.59318864],\n",
       "       [0.57407093],\n",
       "       [0.5691657 ],\n",
       "       [0.4645611 ],\n",
       "       [0.58990705],\n",
       "       [0.55289084],\n",
       "       [0.57831395],\n",
       "       [0.45667452],\n",
       "       [0.49235702],\n",
       "       [0.4629368 ],\n",
       "       [0.5560399 ],\n",
       "       [0.5604906 ],\n",
       "       [0.50133485],\n",
       "       [0.47855425],\n",
       "       [0.5462953 ],\n",
       "       [0.47129774],\n",
       "       [0.4526096 ],\n",
       "       [0.5059497 ],\n",
       "       [0.6364045 ],\n",
       "       [0.4673803 ],\n",
       "       [0.503598  ],\n",
       "       [0.6082289 ],\n",
       "       [0.57993656],\n",
       "       [0.49199745],\n",
       "       [0.5517266 ],\n",
       "       [0.58824617],\n",
       "       [0.57437575],\n",
       "       [0.5684785 ],\n",
       "       [0.52075154],\n",
       "       [0.44932556],\n",
       "       [0.6055098 ],\n",
       "       [0.49391666],\n",
       "       [0.513465  ],\n",
       "       [0.5472597 ],\n",
       "       [0.5304972 ],\n",
       "       [0.49353576],\n",
       "       [0.60886127],\n",
       "       [0.5469754 ],\n",
       "       [0.5220818 ],\n",
       "       [0.54251546],\n",
       "       [0.611508  ],\n",
       "       [0.4261251 ],\n",
       "       [0.59051365],\n",
       "       [0.6464554 ],\n",
       "       [0.45837206],\n",
       "       [0.5470321 ],\n",
       "       [0.6190693 ],\n",
       "       [0.5024035 ],\n",
       "       [0.50624245],\n",
       "       [0.49861866],\n",
       "       [0.54507077],\n",
       "       [0.4990829 ],\n",
       "       [0.5235784 ],\n",
       "       [0.5807462 ],\n",
       "       [0.5697134 ],\n",
       "       [0.5489267 ],\n",
       "       [0.5725117 ],\n",
       "       [0.52625406],\n",
       "       [0.572174  ],\n",
       "       [0.5555788 ],\n",
       "       [0.53971434],\n",
       "       [0.5252868 ],\n",
       "       [0.5067075 ],\n",
       "       [0.53015804],\n",
       "       [0.55012846],\n",
       "       [0.6105386 ],\n",
       "       [0.511522  ],\n",
       "       [0.5949117 ],\n",
       "       [0.5764131 ],\n",
       "       [0.58377403],\n",
       "       [0.5279829 ],\n",
       "       [0.5149169 ],\n",
       "       [0.47046423],\n",
       "       [0.5329815 ],\n",
       "       [0.6094165 ],\n",
       "       [0.5672373 ],\n",
       "       [0.5243069 ],\n",
       "       [0.51794416],\n",
       "       [0.50148726],\n",
       "       [0.4728248 ],\n",
       "       [0.6005068 ],\n",
       "       [0.49859703],\n",
       "       [0.50642836],\n",
       "       [0.5101977 ],\n",
       "       [0.5721061 ],\n",
       "       [0.5220819 ],\n",
       "       [0.51395833],\n",
       "       [0.5495613 ],\n",
       "       [0.60244495],\n",
       "       [0.62404734],\n",
       "       [0.5403846 ],\n",
       "       [0.616015  ],\n",
       "       [0.5759773 ],\n",
       "       [0.5168113 ],\n",
       "       [0.520461  ],\n",
       "       [0.6267277 ],\n",
       "       [0.5468766 ],\n",
       "       [0.58760345],\n",
       "       [0.5196068 ],\n",
       "       [0.5482058 ],\n",
       "       [0.4749832 ],\n",
       "       [0.5751384 ],\n",
       "       [0.5068321 ],\n",
       "       [0.6020721 ],\n",
       "       [0.49987176],\n",
       "       [0.56046855],\n",
       "       [0.5390516 ],\n",
       "       [0.53076416],\n",
       "       [0.5332156 ],\n",
       "       [0.47722495],\n",
       "       [0.5329174 ],\n",
       "       [0.47950217],\n",
       "       [0.51690245],\n",
       "       [0.4877692 ],\n",
       "       [0.55204755],\n",
       "       [0.52900285],\n",
       "       [0.48565376],\n",
       "       [0.57364637],\n",
       "       [0.4494052 ],\n",
       "       [0.57126117],\n",
       "       [0.5234139 ],\n",
       "       [0.6070811 ],\n",
       "       [0.52627665],\n",
       "       [0.5752254 ],\n",
       "       [0.5556719 ],\n",
       "       [0.42572877],\n",
       "       [0.49283326],\n",
       "       [0.49846894],\n",
       "       [0.52661383],\n",
       "       [0.45755482],\n",
       "       [0.55880505],\n",
       "       [0.54349214],\n",
       "       [0.558785  ],\n",
       "       [0.54171646],\n",
       "       [0.5045004 ],\n",
       "       [0.53251344],\n",
       "       [0.5296866 ],\n",
       "       [0.5135674 ],\n",
       "       [0.48141757],\n",
       "       [0.6835735 ],\n",
       "       [0.48169443],\n",
       "       [0.5377821 ],\n",
       "       [0.5824793 ],\n",
       "       [0.5347215 ],\n",
       "       [0.48480746],\n",
       "       [0.58016324],\n",
       "       [0.5774252 ],\n",
       "       [0.62061524],\n",
       "       [0.5025304 ],\n",
       "       [0.4845174 ],\n",
       "       [0.47453812],\n",
       "       [0.49153793],\n",
       "       [0.55965275],\n",
       "       [0.4704197 ],\n",
       "       [0.47269025],\n",
       "       [0.51491755],\n",
       "       [0.63495445],\n",
       "       [0.5550201 ],\n",
       "       [0.5324388 ],\n",
       "       [0.50797343],\n",
       "       [0.5435967 ],\n",
       "       [0.5002666 ],\n",
       "       [0.600915  ],\n",
       "       [0.5442337 ],\n",
       "       [0.5190421 ],\n",
       "       [0.5862533 ],\n",
       "       [0.43055338],\n",
       "       [0.614223  ],\n",
       "       [0.50454235],\n",
       "       [0.50464123],\n",
       "       [0.4995938 ],\n",
       "       [0.62375724],\n",
       "       [0.49777824],\n",
       "       [0.5566958 ],\n",
       "       [0.6273437 ],\n",
       "       [0.4399287 ],\n",
       "       [0.516439  ],\n",
       "       [0.52047116],\n",
       "       [0.47241405],\n",
       "       [0.6240671 ],\n",
       "       [0.55531174],\n",
       "       [0.52105355],\n",
       "       [0.43517315],\n",
       "       [0.5968252 ],\n",
       "       [0.47214606],\n",
       "       [0.5723824 ],\n",
       "       [0.48934138],\n",
       "       [0.5710443 ],\n",
       "       [0.58340365],\n",
       "       [0.59924054],\n",
       "       [0.4915202 ],\n",
       "       [0.5429754 ],\n",
       "       [0.5321423 ],\n",
       "       [0.5340592 ],\n",
       "       [0.58086586],\n",
       "       [0.5176376 ],\n",
       "       [0.48329934],\n",
       "       [0.5265461 ],\n",
       "       [0.5434443 ],\n",
       "       [0.57995546],\n",
       "       [0.45440915],\n",
       "       [0.51469356],\n",
       "       [0.46747175],\n",
       "       [0.4798045 ],\n",
       "       [0.54771906],\n",
       "       [0.5483517 ],\n",
       "       [0.5852723 ],\n",
       "       [0.60030913],\n",
       "       [0.49232832],\n",
       "       [0.5121715 ],\n",
       "       [0.5246455 ],\n",
       "       [0.5017493 ],\n",
       "       [0.5646295 ],\n",
       "       [0.4844697 ],\n",
       "       [0.49964896],\n",
       "       [0.5539731 ],\n",
       "       [0.5280769 ],\n",
       "       [0.5296947 ],\n",
       "       [0.6358349 ],\n",
       "       [0.5494626 ],\n",
       "       [0.47350743],\n",
       "       [0.44118297],\n",
       "       [0.4042386 ],\n",
       "       [0.4089081 ],\n",
       "       [0.595505  ],\n",
       "       [0.5057993 ],\n",
       "       [0.6143968 ],\n",
       "       [0.55171424],\n",
       "       [0.5871034 ],\n",
       "       [0.41795677],\n",
       "       [0.4310805 ],\n",
       "       [0.5873242 ],\n",
       "       [0.5591289 ],\n",
       "       [0.5125399 ],\n",
       "       [0.4765651 ],\n",
       "       [0.4637421 ],\n",
       "       [0.51989174],\n",
       "       [0.5092983 ],\n",
       "       [0.5076269 ],\n",
       "       [0.49689957],\n",
       "       [0.44151804],\n",
       "       [0.46996248],\n",
       "       [0.5807039 ],\n",
       "       [0.5211468 ],\n",
       "       [0.5555485 ],\n",
       "       [0.5279864 ],\n",
       "       [0.5395702 ],\n",
       "       [0.5218049 ],\n",
       "       [0.6387617 ],\n",
       "       [0.5353662 ],\n",
       "       [0.58987993],\n",
       "       [0.5446512 ],\n",
       "       [0.5778339 ],\n",
       "       [0.5075774 ],\n",
       "       [0.5575303 ],\n",
       "       [0.49944678],\n",
       "       [0.6568996 ],\n",
       "       [0.4741121 ],\n",
       "       [0.4644196 ],\n",
       "       [0.43373057],\n",
       "       [0.5730153 ],\n",
       "       [0.579244  ],\n",
       "       [0.5376222 ],\n",
       "       [0.5816078 ],\n",
       "       [0.525045  ],\n",
       "       [0.46527216],\n",
       "       [0.5261661 ],\n",
       "       [0.4448437 ],\n",
       "       [0.4808344 ],\n",
       "       [0.62273085],\n",
       "       [0.5015591 ],\n",
       "       [0.6155071 ],\n",
       "       [0.67318547],\n",
       "       [0.70883465],\n",
       "       [0.51796764],\n",
       "       [0.6010208 ],\n",
       "       [0.4536706 ],\n",
       "       [0.54753476],\n",
       "       [0.51547474],\n",
       "       [0.5055228 ],\n",
       "       [0.5084932 ],\n",
       "       [0.6421423 ],\n",
       "       [0.4879325 ],\n",
       "       [0.606037  ],\n",
       "       [0.5354152 ],\n",
       "       [0.60421354],\n",
       "       [0.4520248 ],\n",
       "       [0.502346  ],\n",
       "       [0.6191002 ],\n",
       "       [0.47537738],\n",
       "       [0.5716187 ],\n",
       "       [0.46267104],\n",
       "       [0.5809953 ],\n",
       "       [0.52593845],\n",
       "       [0.6248851 ],\n",
       "       [0.49872297],\n",
       "       [0.5517892 ],\n",
       "       [0.6451766 ],\n",
       "       [0.46069402],\n",
       "       [0.56356126],\n",
       "       [0.4973791 ],\n",
       "       [0.35377616],\n",
       "       [0.65917563],\n",
       "       [0.53691524],\n",
       "       [0.6623652 ],\n",
       "       [0.5410266 ],\n",
       "       [0.55209434],\n",
       "       [0.6510383 ],\n",
       "       [0.55845654],\n",
       "       [0.5806242 ],\n",
       "       [0.5927398 ],\n",
       "       [0.6234597 ],\n",
       "       [0.49145254],\n",
       "       [0.59908473],\n",
       "       [0.49168155],\n",
       "       [0.6115434 ],\n",
       "       [0.46608236],\n",
       "       [0.5416068 ],\n",
       "       [0.5285821 ],\n",
       "       [0.58596843],\n",
       "       [0.54950273],\n",
       "       [0.5635744 ],\n",
       "       [0.535547  ],\n",
       "       [0.5241493 ],\n",
       "       [0.522915  ],\n",
       "       [0.5009923 ],\n",
       "       [0.51909447],\n",
       "       [0.43996218],\n",
       "       [0.49176762],\n",
       "       [0.5312508 ],\n",
       "       [0.53256935],\n",
       "       [0.52161354],\n",
       "       [0.4890744 ],\n",
       "       [0.5698382 ],\n",
       "       [0.56181055],\n",
       "       [0.47402617],\n",
       "       [0.52340597],\n",
       "       [0.47045606],\n",
       "       [0.5468891 ],\n",
       "       [0.47588667],\n",
       "       [0.54773176],\n",
       "       [0.5895464 ],\n",
       "       [0.51412666],\n",
       "       [0.58095336],\n",
       "       [0.5953051 ],\n",
       "       [0.5110231 ],\n",
       "       [0.54981667],\n",
       "       [0.5092184 ],\n",
       "       [0.5239954 ],\n",
       "       [0.5929474 ],\n",
       "       [0.55197525],\n",
       "       [0.5181829 ],\n",
       "       [0.5113926 ],\n",
       "       [0.53121704],\n",
       "       [0.4739324 ],\n",
       "       [0.51801026],\n",
       "       [0.5294143 ],\n",
       "       [0.5243701 ],\n",
       "       [0.61109126],\n",
       "       [0.51620907],\n",
       "       [0.5650225 ],\n",
       "       [0.46491274],\n",
       "       [0.5011474 ],\n",
       "       [0.53302014],\n",
       "       [0.51112396],\n",
       "       [0.5985587 ],\n",
       "       [0.56975543],\n",
       "       [0.47190642],\n",
       "       [0.45356748],\n",
       "       [0.5843006 ],\n",
       "       [0.5071279 ],\n",
       "       [0.60662377],\n",
       "       [0.51431054],\n",
       "       [0.6910016 ],\n",
       "       [0.5878929 ],\n",
       "       [0.60999155],\n",
       "       [0.47708532],\n",
       "       [0.5895671 ],\n",
       "       [0.6148523 ],\n",
       "       [0.5470098 ],\n",
       "       [0.5832223 ],\n",
       "       [0.4720833 ],\n",
       "       [0.5975123 ],\n",
       "       [0.45697007],\n",
       "       [0.42744276],\n",
       "       [0.5918934 ],\n",
       "       [0.58575267],\n",
       "       [0.56779367],\n",
       "       [0.44680154],\n",
       "       [0.5098    ],\n",
       "       [0.60305566],\n",
       "       [0.5835604 ],\n",
       "       [0.55349576],\n",
       "       [0.5580473 ],\n",
       "       [0.46859205],\n",
       "       [0.61443084],\n",
       "       [0.5505495 ],\n",
       "       [0.4991864 ],\n",
       "       [0.4578976 ],\n",
       "       [0.6157431 ],\n",
       "       [0.70493495],\n",
       "       [0.43956092],\n",
       "       [0.5782771 ],\n",
       "       [0.49954847],\n",
       "       [0.5993042 ],\n",
       "       [0.535066  ],\n",
       "       [0.47370914],\n",
       "       [0.5786347 ],\n",
       "       [0.5611995 ],\n",
       "       [0.5428848 ],\n",
       "       [0.44056618],\n",
       "       [0.52862495],\n",
       "       [0.5210259 ],\n",
       "       [0.528264  ],\n",
       "       [0.6157591 ],\n",
       "       [0.4644041 ],\n",
       "       [0.51831824],\n",
       "       [0.58534896],\n",
       "       [0.39274758],\n",
       "       [0.5451313 ],\n",
       "       [0.5630662 ],\n",
       "       [0.5760573 ],\n",
       "       [0.61181486],\n",
       "       [0.57242584],\n",
       "       [0.48978624],\n",
       "       [0.53362733],\n",
       "       [0.56235427],\n",
       "       [0.5367067 ],\n",
       "       [0.63123447],\n",
       "       [0.5161089 ],\n",
       "       [0.5535976 ],\n",
       "       [0.6178333 ],\n",
       "       [0.4862375 ],\n",
       "       [0.56897545],\n",
       "       [0.48723692],\n",
       "       [0.6336695 ],\n",
       "       [0.5067313 ],\n",
       "       [0.5163639 ],\n",
       "       [0.4874879 ],\n",
       "       [0.494709  ],\n",
       "       [0.49238116],\n",
       "       [0.5954396 ],\n",
       "       [0.65389574],\n",
       "       [0.52142763],\n",
       "       [0.5824534 ],\n",
       "       [0.49554893],\n",
       "       [0.586097  ],\n",
       "       [0.53590846],\n",
       "       [0.49260154],\n",
       "       [0.60668623],\n",
       "       [0.50820273],\n",
       "       [0.5436338 ],\n",
       "       [0.56599724],\n",
       "       [0.4955972 ],\n",
       "       [0.58242327],\n",
       "       [0.5058291 ],\n",
       "       [0.6253167 ],\n",
       "       [0.5437041 ],\n",
       "       [0.42282766],\n",
       "       [0.47833082],\n",
       "       [0.6299679 ],\n",
       "       [0.6557714 ],\n",
       "       [0.42474028],\n",
       "       [0.63450265],\n",
       "       [0.5921606 ],\n",
       "       [0.4796418 ],\n",
       "       [0.51710176],\n",
       "       [0.4791194 ],\n",
       "       [0.3873628 ],\n",
       "       [0.55507916],\n",
       "       [0.48148265],\n",
       "       [0.4530567 ],\n",
       "       [0.5119491 ],\n",
       "       [0.5952159 ],\n",
       "       [0.6301497 ],\n",
       "       [0.51324207],\n",
       "       [0.62386894],\n",
       "       [0.6294781 ],\n",
       "       [0.54643935],\n",
       "       [0.6373199 ],\n",
       "       [0.43753204],\n",
       "       [0.5064964 ],\n",
       "       [0.4576676 ],\n",
       "       [0.39543408],\n",
       "       [0.5359108 ],\n",
       "       [0.59900963],\n",
       "       [0.52454275],\n",
       "       [0.44269034],\n",
       "       [0.5092987 ],\n",
       "       [0.5751498 ],\n",
       "       [0.57598484],\n",
       "       [0.5564311 ],\n",
       "       [0.44365227],\n",
       "       [0.5758392 ],\n",
       "       [0.5522732 ],\n",
       "       [0.5287346 ],\n",
       "       [0.5862236 ],\n",
       "       [0.61913943],\n",
       "       [0.49446073],\n",
       "       [0.50724906],\n",
       "       [0.57449627],\n",
       "       [0.57661146],\n",
       "       [0.6451338 ],\n",
       "       [0.6529244 ],\n",
       "       [0.64387655],\n",
       "       [0.57414365],\n",
       "       [0.51640326],\n",
       "       [0.57317156],\n",
       "       [0.50558686],\n",
       "       [0.53334546],\n",
       "       [0.4571467 ],\n",
       "       [0.53343946],\n",
       "       [0.47204658],\n",
       "       [0.44472829],\n",
       "       [0.57409644],\n",
       "       [0.5380021 ],\n",
       "       [0.52514434],\n",
       "       [0.50623804],\n",
       "       [0.57499313],\n",
       "       [0.5948829 ],\n",
       "       [0.47079757],\n",
       "       [0.5410575 ],\n",
       "       [0.55049187],\n",
       "       [0.61353415],\n",
       "       [0.54181564],\n",
       "       [0.48442242],\n",
       "       [0.5594523 ],\n",
       "       [0.6715901 ],\n",
       "       [0.4731269 ],\n",
       "       [0.6228464 ],\n",
       "       [0.5218674 ],\n",
       "       [0.56970495],\n",
       "       [0.578103  ],\n",
       "       [0.5837587 ],\n",
       "       [0.5313679 ],\n",
       "       [0.5168598 ],\n",
       "       [0.61218673],\n",
       "       [0.4043343 ],\n",
       "       [0.4414801 ],\n",
       "       [0.49897647],\n",
       "       [0.49822605],\n",
       "       [0.49480423],\n",
       "       [0.538377  ],\n",
       "       [0.49550807],\n",
       "       [0.5591653 ],\n",
       "       [0.558627  ],\n",
       "       [0.5113084 ],\n",
       "       [0.43448856],\n",
       "       [0.59963244],\n",
       "       [0.5210182 ],\n",
       "       [0.533172  ],\n",
       "       [0.56277204],\n",
       "       [0.46209013],\n",
       "       [0.4710573 ],\n",
       "       [0.38567305],\n",
       "       [0.51867205],\n",
       "       [0.49150616],\n",
       "       [0.59228665],\n",
       "       [0.5062874 ],\n",
       "       [0.50683314],\n",
       "       [0.5984997 ],\n",
       "       [0.47163212],\n",
       "       [0.627573  ],\n",
       "       [0.5060715 ],\n",
       "       [0.57554126],\n",
       "       [0.5570886 ],\n",
       "       [0.63501424],\n",
       "       [0.55859256],\n",
       "       [0.5182954 ],\n",
       "       [0.61613214],\n",
       "       [0.57871556],\n",
       "       [0.65181947],\n",
       "       [0.5130813 ],\n",
       "       [0.51666534],\n",
       "       [0.5755763 ],\n",
       "       [0.46981353],\n",
       "       [0.4794296 ],\n",
       "       [0.61046386],\n",
       "       [0.50077134],\n",
       "       [0.5260161 ],\n",
       "       [0.509824  ],\n",
       "       [0.5870474 ],\n",
       "       [0.46008503],\n",
       "       [0.4199245 ],\n",
       "       [0.5363921 ],\n",
       "       [0.59281635],\n",
       "       [0.575356  ],\n",
       "       [0.56464124],\n",
       "       [0.5452522 ],\n",
       "       [0.51201886],\n",
       "       [0.60757613],\n",
       "       [0.5942029 ],\n",
       "       [0.45369   ],\n",
       "       [0.52265155],\n",
       "       [0.56694514],\n",
       "       [0.5213943 ],\n",
       "       [0.4850337 ],\n",
       "       [0.5676468 ],\n",
       "       [0.5268705 ],\n",
       "       [0.59407943],\n",
       "       [0.6117222 ],\n",
       "       [0.5608104 ],\n",
       "       [0.5534297 ],\n",
       "       [0.46101907],\n",
       "       [0.4982161 ],\n",
       "       [0.50742483],\n",
       "       [0.5004167 ],\n",
       "       [0.6143202 ],\n",
       "       [0.42979562],\n",
       "       [0.62508357],\n",
       "       [0.5539253 ],\n",
       "       [0.50585234],\n",
       "       [0.40970463],\n",
       "       [0.54990333],\n",
       "       [0.563866  ],\n",
       "       [0.50632375],\n",
       "       [0.51598644],\n",
       "       [0.5210067 ],\n",
       "       [0.59085673],\n",
       "       [0.543144  ],\n",
       "       [0.57504916],\n",
       "       [0.5370952 ],\n",
       "       [0.5525577 ],\n",
       "       [0.53333455],\n",
       "       [0.504937  ],\n",
       "       [0.51820517],\n",
       "       [0.48223603],\n",
       "       [0.54376775],\n",
       "       [0.5863325 ],\n",
       "       [0.5997209 ],\n",
       "       [0.5945756 ],\n",
       "       [0.55737805],\n",
       "       [0.52227414],\n",
       "       [0.5119497 ],\n",
       "       [0.57944894],\n",
       "       [0.61345744],\n",
       "       [0.5417664 ],\n",
       "       [0.49240512],\n",
       "       [0.5532928 ],\n",
       "       [0.50778437],\n",
       "       [0.5198715 ],\n",
       "       [0.426161  ],\n",
       "       [0.5627617 ],\n",
       "       [0.6082178 ],\n",
       "       [0.61154336],\n",
       "       [0.57350147],\n",
       "       [0.5932696 ],\n",
       "       [0.58516574],\n",
       "       [0.6917117 ],\n",
       "       [0.5734081 ],\n",
       "       [0.58616096],\n",
       "       [0.46247897],\n",
       "       [0.5868168 ],\n",
       "       [0.48684153],\n",
       "       [0.5885721 ],\n",
       "       [0.46887618],\n",
       "       [0.545781  ],\n",
       "       [0.4723081 ],\n",
       "       [0.5025746 ],\n",
       "       [0.5798459 ],\n",
       "       [0.47238454]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\n",
    "            data,\n",
    "            batch_size=32,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-53-78e567ba69ae>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "32/32 [==============================] - 0s 415us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(\n",
    "            data,\n",
    "            batch_size=32,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-55-399dd2eb4f73>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "32/32 [==============================] - 0s 632us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.58177596],\n",
       "       [0.51481676],\n",
       "       [0.48363435],\n",
       "       [0.37324744],\n",
       "       [0.5909763 ],\n",
       "       [0.5085412 ],\n",
       "       [0.5756633 ],\n",
       "       [0.5098746 ],\n",
       "       [0.43131375],\n",
       "       [0.58133817],\n",
       "       [0.5892435 ],\n",
       "       [0.54077   ],\n",
       "       [0.5013192 ],\n",
       "       [0.5410947 ],\n",
       "       [0.62009776],\n",
       "       [0.34385467],\n",
       "       [0.51875365],\n",
       "       [0.5697975 ],\n",
       "       [0.47216827],\n",
       "       [0.6092778 ],\n",
       "       [0.41083294],\n",
       "       [0.58048546],\n",
       "       [0.5323728 ],\n",
       "       [0.5453252 ],\n",
       "       [0.48651725],\n",
       "       [0.55320704],\n",
       "       [0.6096177 ],\n",
       "       [0.5063046 ],\n",
       "       [0.5143441 ],\n",
       "       [0.51943004],\n",
       "       [0.5602287 ],\n",
       "       [0.5128446 ],\n",
       "       [0.56864613],\n",
       "       [0.5354724 ],\n",
       "       [0.5388147 ],\n",
       "       [0.54790336],\n",
       "       [0.53065485],\n",
       "       [0.44630828],\n",
       "       [0.5328061 ],\n",
       "       [0.65760213],\n",
       "       [0.46917966],\n",
       "       [0.45668143],\n",
       "       [0.5905872 ],\n",
       "       [0.45381984],\n",
       "       [0.5425263 ],\n",
       "       [0.60115963],\n",
       "       [0.4705356 ],\n",
       "       [0.6448649 ],\n",
       "       [0.5805385 ],\n",
       "       [0.49101174],\n",
       "       [0.604499  ],\n",
       "       [0.48229808],\n",
       "       [0.4947881 ],\n",
       "       [0.61006194],\n",
       "       [0.6359745 ],\n",
       "       [0.6629744 ],\n",
       "       [0.62611383],\n",
       "       [0.4822327 ],\n",
       "       [0.48648828],\n",
       "       [0.5855813 ],\n",
       "       [0.61858004],\n",
       "       [0.49099502],\n",
       "       [0.44074893],\n",
       "       [0.5807049 ],\n",
       "       [0.5826957 ],\n",
       "       [0.5644451 ],\n",
       "       [0.37011513],\n",
       "       [0.5609652 ],\n",
       "       [0.5569893 ],\n",
       "       [0.56548595],\n",
       "       [0.557604  ],\n",
       "       [0.5371765 ],\n",
       "       [0.59912646],\n",
       "       [0.5704123 ],\n",
       "       [0.49818105],\n",
       "       [0.63446045],\n",
       "       [0.42390922],\n",
       "       [0.5950625 ],\n",
       "       [0.59411985],\n",
       "       [0.6404207 ],\n",
       "       [0.55670804],\n",
       "       [0.55618924],\n",
       "       [0.59857   ],\n",
       "       [0.6043004 ],\n",
       "       [0.5565191 ],\n",
       "       [0.5976366 ],\n",
       "       [0.5248617 ],\n",
       "       [0.5140953 ],\n",
       "       [0.5356883 ],\n",
       "       [0.59041286],\n",
       "       [0.57305956],\n",
       "       [0.5530154 ],\n",
       "       [0.5540613 ],\n",
       "       [0.60329354],\n",
       "       [0.5164435 ],\n",
       "       [0.5365882 ],\n",
       "       [0.57955545],\n",
       "       [0.4717487 ],\n",
       "       [0.5814121 ],\n",
       "       [0.534452  ],\n",
       "       [0.5613296 ],\n",
       "       [0.5201451 ],\n",
       "       [0.56531155],\n",
       "       [0.5396855 ],\n",
       "       [0.45899957],\n",
       "       [0.49455723],\n",
       "       [0.5343444 ],\n",
       "       [0.4909596 ],\n",
       "       [0.4867859 ],\n",
       "       [0.5226418 ],\n",
       "       [0.5404243 ],\n",
       "       [0.5524246 ],\n",
       "       [0.43250236],\n",
       "       [0.50478125],\n",
       "       [0.5375989 ],\n",
       "       [0.5398732 ],\n",
       "       [0.6630514 ],\n",
       "       [0.57632977],\n",
       "       [0.49661553],\n",
       "       [0.5053761 ],\n",
       "       [0.44295838],\n",
       "       [0.5545483 ],\n",
       "       [0.6353956 ],\n",
       "       [0.5358864 ],\n",
       "       [0.5833692 ],\n",
       "       [0.6797298 ],\n",
       "       [0.42872292],\n",
       "       [0.51514864],\n",
       "       [0.57596046],\n",
       "       [0.5506141 ],\n",
       "       [0.56543565],\n",
       "       [0.503982  ],\n",
       "       [0.50474745],\n",
       "       [0.52629536],\n",
       "       [0.42236412],\n",
       "       [0.46824867],\n",
       "       [0.52257746],\n",
       "       [0.56195724],\n",
       "       [0.4597649 ],\n",
       "       [0.56791365],\n",
       "       [0.4607302 ],\n",
       "       [0.62795705],\n",
       "       [0.5715778 ],\n",
       "       [0.53286666],\n",
       "       [0.529911  ],\n",
       "       [0.51806897],\n",
       "       [0.5414875 ],\n",
       "       [0.56459546],\n",
       "       [0.42341202],\n",
       "       [0.5905674 ],\n",
       "       [0.5442033 ],\n",
       "       [0.53160554],\n",
       "       [0.5723476 ],\n",
       "       [0.48176584],\n",
       "       [0.6103639 ],\n",
       "       [0.5138205 ],\n",
       "       [0.6138399 ],\n",
       "       [0.5771618 ],\n",
       "       [0.5275352 ],\n",
       "       [0.5159543 ],\n",
       "       [0.6319519 ],\n",
       "       [0.549439  ],\n",
       "       [0.5761145 ],\n",
       "       [0.42865372],\n",
       "       [0.2963707 ],\n",
       "       [0.4671441 ],\n",
       "       [0.5639633 ],\n",
       "       [0.45143533],\n",
       "       [0.4854626 ],\n",
       "       [0.4555644 ],\n",
       "       [0.49173385],\n",
       "       [0.55257595],\n",
       "       [0.6043271 ],\n",
       "       [0.60038567],\n",
       "       [0.6033316 ],\n",
       "       [0.52967083],\n",
       "       [0.651252  ],\n",
       "       [0.5220652 ],\n",
       "       [0.52825046],\n",
       "       [0.4749895 ],\n",
       "       [0.50582707],\n",
       "       [0.5101579 ],\n",
       "       [0.5283649 ],\n",
       "       [0.49244675],\n",
       "       [0.54641134],\n",
       "       [0.61158615],\n",
       "       [0.48008758],\n",
       "       [0.44878078],\n",
       "       [0.5303739 ],\n",
       "       [0.44558448],\n",
       "       [0.6981847 ],\n",
       "       [0.4773438 ],\n",
       "       [0.46339783],\n",
       "       [0.47132185],\n",
       "       [0.5691614 ],\n",
       "       [0.4920636 ],\n",
       "       [0.53800195],\n",
       "       [0.5641241 ],\n",
       "       [0.5183179 ],\n",
       "       [0.5719949 ],\n",
       "       [0.5457534 ],\n",
       "       [0.6215098 ],\n",
       "       [0.54857165],\n",
       "       [0.5396625 ],\n",
       "       [0.55748767],\n",
       "       [0.52246034],\n",
       "       [0.66210085],\n",
       "       [0.63866097],\n",
       "       [0.47768265],\n",
       "       [0.57954276],\n",
       "       [0.58833146],\n",
       "       [0.5379796 ],\n",
       "       [0.5024652 ],\n",
       "       [0.47586742],\n",
       "       [0.46542624],\n",
       "       [0.51874405],\n",
       "       [0.62522334],\n",
       "       [0.56035095],\n",
       "       [0.3777146 ],\n",
       "       [0.5646743 ],\n",
       "       [0.6112769 ],\n",
       "       [0.4531964 ],\n",
       "       [0.4982675 ],\n",
       "       [0.5277441 ],\n",
       "       [0.5864322 ],\n",
       "       [0.52577597],\n",
       "       [0.5386518 ],\n",
       "       [0.51091284],\n",
       "       [0.4926977 ],\n",
       "       [0.46560982],\n",
       "       [0.62024593],\n",
       "       [0.59142625],\n",
       "       [0.5183876 ],\n",
       "       [0.57075745],\n",
       "       [0.54012966],\n",
       "       [0.55466896],\n",
       "       [0.605686  ],\n",
       "       [0.5136309 ],\n",
       "       [0.5631776 ],\n",
       "       [0.6795104 ],\n",
       "       [0.48870328],\n",
       "       [0.5163    ],\n",
       "       [0.49844962],\n",
       "       [0.5902507 ],\n",
       "       [0.5404836 ],\n",
       "       [0.62408644],\n",
       "       [0.4005914 ],\n",
       "       [0.46308422],\n",
       "       [0.59380287],\n",
       "       [0.4356421 ],\n",
       "       [0.5241218 ],\n",
       "       [0.5203286 ],\n",
       "       [0.54666615],\n",
       "       [0.50199443],\n",
       "       [0.54593974],\n",
       "       [0.5621558 ],\n",
       "       [0.515104  ],\n",
       "       [0.44401678],\n",
       "       [0.5033203 ],\n",
       "       [0.5629863 ],\n",
       "       [0.4378238 ],\n",
       "       [0.3930524 ],\n",
       "       [0.47931904],\n",
       "       [0.55965513],\n",
       "       [0.5597145 ],\n",
       "       [0.5273316 ],\n",
       "       [0.5123346 ],\n",
       "       [0.5956402 ],\n",
       "       [0.5634515 ],\n",
       "       [0.44600934],\n",
       "       [0.64536667],\n",
       "       [0.5500321 ],\n",
       "       [0.49892956],\n",
       "       [0.6558354 ],\n",
       "       [0.5087409 ],\n",
       "       [0.561794  ],\n",
       "       [0.5413153 ],\n",
       "       [0.5303111 ],\n",
       "       [0.54642045],\n",
       "       [0.56814176],\n",
       "       [0.4971294 ],\n",
       "       [0.63997084],\n",
       "       [0.58798325],\n",
       "       [0.5236035 ],\n",
       "       [0.52853596],\n",
       "       [0.558397  ],\n",
       "       [0.5316837 ],\n",
       "       [0.5090827 ],\n",
       "       [0.5265963 ],\n",
       "       [0.4307574 ],\n",
       "       [0.53356546],\n",
       "       [0.63312227],\n",
       "       [0.48293036],\n",
       "       [0.57629013],\n",
       "       [0.55753785],\n",
       "       [0.5965842 ],\n",
       "       [0.45748514],\n",
       "       [0.5279471 ],\n",
       "       [0.6353927 ],\n",
       "       [0.6709193 ],\n",
       "       [0.5581989 ],\n",
       "       [0.6300532 ],\n",
       "       [0.5797044 ],\n",
       "       [0.5196977 ],\n",
       "       [0.55459887],\n",
       "       [0.49245209],\n",
       "       [0.5504899 ],\n",
       "       [0.5054472 ],\n",
       "       [0.50395614],\n",
       "       [0.50078934],\n",
       "       [0.55569804],\n",
       "       [0.45976275],\n",
       "       [0.5187669 ],\n",
       "       [0.5249917 ],\n",
       "       [0.66540045],\n",
       "       [0.6396723 ],\n",
       "       [0.475132  ],\n",
       "       [0.5727713 ],\n",
       "       [0.55391085],\n",
       "       [0.56901026],\n",
       "       [0.48504874],\n",
       "       [0.6461092 ],\n",
       "       [0.6623888 ],\n",
       "       [0.55323166],\n",
       "       [0.53539205],\n",
       "       [0.62378883],\n",
       "       [0.5527836 ],\n",
       "       [0.5138852 ],\n",
       "       [0.6103897 ],\n",
       "       [0.5860998 ],\n",
       "       [0.50941384],\n",
       "       [0.56736475],\n",
       "       [0.5443313 ],\n",
       "       [0.5338296 ],\n",
       "       [0.57146394],\n",
       "       [0.5055846 ],\n",
       "       [0.5293094 ],\n",
       "       [0.57135564],\n",
       "       [0.55058515],\n",
       "       [0.59318864],\n",
       "       [0.57407093],\n",
       "       [0.5691657 ],\n",
       "       [0.4645611 ],\n",
       "       [0.58990705],\n",
       "       [0.55289084],\n",
       "       [0.57831395],\n",
       "       [0.45667452],\n",
       "       [0.49235702],\n",
       "       [0.4629368 ],\n",
       "       [0.5560399 ],\n",
       "       [0.5604906 ],\n",
       "       [0.50133485],\n",
       "       [0.47855425],\n",
       "       [0.5462953 ],\n",
       "       [0.47129774],\n",
       "       [0.4526096 ],\n",
       "       [0.5059497 ],\n",
       "       [0.6364045 ],\n",
       "       [0.4673803 ],\n",
       "       [0.503598  ],\n",
       "       [0.6082289 ],\n",
       "       [0.57993656],\n",
       "       [0.49199745],\n",
       "       [0.5517266 ],\n",
       "       [0.58824617],\n",
       "       [0.57437575],\n",
       "       [0.5684785 ],\n",
       "       [0.52075154],\n",
       "       [0.44932556],\n",
       "       [0.6055098 ],\n",
       "       [0.49391666],\n",
       "       [0.513465  ],\n",
       "       [0.5472597 ],\n",
       "       [0.5304972 ],\n",
       "       [0.49353576],\n",
       "       [0.60886127],\n",
       "       [0.5469754 ],\n",
       "       [0.5220818 ],\n",
       "       [0.54251546],\n",
       "       [0.611508  ],\n",
       "       [0.4261251 ],\n",
       "       [0.59051365],\n",
       "       [0.6464554 ],\n",
       "       [0.45837206],\n",
       "       [0.5470321 ],\n",
       "       [0.6190693 ],\n",
       "       [0.5024035 ],\n",
       "       [0.50624245],\n",
       "       [0.49861866],\n",
       "       [0.54507077],\n",
       "       [0.4990829 ],\n",
       "       [0.5235784 ],\n",
       "       [0.5807462 ],\n",
       "       [0.5697134 ],\n",
       "       [0.5489267 ],\n",
       "       [0.5725117 ],\n",
       "       [0.52625406],\n",
       "       [0.572174  ],\n",
       "       [0.5555788 ],\n",
       "       [0.53971434],\n",
       "       [0.5252868 ],\n",
       "       [0.5067075 ],\n",
       "       [0.53015804],\n",
       "       [0.55012846],\n",
       "       [0.6105386 ],\n",
       "       [0.511522  ],\n",
       "       [0.5949117 ],\n",
       "       [0.5764131 ],\n",
       "       [0.58377403],\n",
       "       [0.5279829 ],\n",
       "       [0.5149169 ],\n",
       "       [0.47046423],\n",
       "       [0.5329815 ],\n",
       "       [0.6094165 ],\n",
       "       [0.5672373 ],\n",
       "       [0.5243069 ],\n",
       "       [0.51794416],\n",
       "       [0.50148726],\n",
       "       [0.4728248 ],\n",
       "       [0.6005068 ],\n",
       "       [0.49859703],\n",
       "       [0.50642836],\n",
       "       [0.5101977 ],\n",
       "       [0.5721061 ],\n",
       "       [0.5220819 ],\n",
       "       [0.51395833],\n",
       "       [0.5495613 ],\n",
       "       [0.60244495],\n",
       "       [0.62404734],\n",
       "       [0.5403846 ],\n",
       "       [0.616015  ],\n",
       "       [0.5759773 ],\n",
       "       [0.5168113 ],\n",
       "       [0.520461  ],\n",
       "       [0.6267277 ],\n",
       "       [0.5468766 ],\n",
       "       [0.58760345],\n",
       "       [0.5196068 ],\n",
       "       [0.5482058 ],\n",
       "       [0.4749832 ],\n",
       "       [0.5751384 ],\n",
       "       [0.5068321 ],\n",
       "       [0.6020721 ],\n",
       "       [0.49987176],\n",
       "       [0.56046855],\n",
       "       [0.5390516 ],\n",
       "       [0.53076416],\n",
       "       [0.5332156 ],\n",
       "       [0.47722495],\n",
       "       [0.5329174 ],\n",
       "       [0.47950217],\n",
       "       [0.51690245],\n",
       "       [0.4877692 ],\n",
       "       [0.55204755],\n",
       "       [0.52900285],\n",
       "       [0.48565376],\n",
       "       [0.57364637],\n",
       "       [0.4494052 ],\n",
       "       [0.57126117],\n",
       "       [0.5234139 ],\n",
       "       [0.6070811 ],\n",
       "       [0.52627665],\n",
       "       [0.5752254 ],\n",
       "       [0.5556719 ],\n",
       "       [0.42572877],\n",
       "       [0.49283326],\n",
       "       [0.49846894],\n",
       "       [0.52661383],\n",
       "       [0.45755482],\n",
       "       [0.55880505],\n",
       "       [0.54349214],\n",
       "       [0.558785  ],\n",
       "       [0.54171646],\n",
       "       [0.5045004 ],\n",
       "       [0.53251344],\n",
       "       [0.5296866 ],\n",
       "       [0.5135674 ],\n",
       "       [0.48141757],\n",
       "       [0.6835735 ],\n",
       "       [0.48169443],\n",
       "       [0.5377821 ],\n",
       "       [0.5824793 ],\n",
       "       [0.5347215 ],\n",
       "       [0.48480746],\n",
       "       [0.58016324],\n",
       "       [0.5774252 ],\n",
       "       [0.62061524],\n",
       "       [0.5025304 ],\n",
       "       [0.4845174 ],\n",
       "       [0.47453812],\n",
       "       [0.49153793],\n",
       "       [0.55965275],\n",
       "       [0.4704197 ],\n",
       "       [0.47269025],\n",
       "       [0.51491755],\n",
       "       [0.63495445],\n",
       "       [0.5550201 ],\n",
       "       [0.5324388 ],\n",
       "       [0.50797343],\n",
       "       [0.5435967 ],\n",
       "       [0.5002666 ],\n",
       "       [0.600915  ],\n",
       "       [0.5442337 ],\n",
       "       [0.5190421 ],\n",
       "       [0.5862533 ],\n",
       "       [0.43055338],\n",
       "       [0.614223  ],\n",
       "       [0.50454235],\n",
       "       [0.50464123],\n",
       "       [0.4995938 ],\n",
       "       [0.62375724],\n",
       "       [0.49777824],\n",
       "       [0.5566958 ],\n",
       "       [0.6273437 ],\n",
       "       [0.4399287 ],\n",
       "       [0.516439  ],\n",
       "       [0.52047116],\n",
       "       [0.47241405],\n",
       "       [0.6240671 ],\n",
       "       [0.55531174],\n",
       "       [0.52105355],\n",
       "       [0.43517315],\n",
       "       [0.5968252 ],\n",
       "       [0.47214606],\n",
       "       [0.5723824 ],\n",
       "       [0.48934138],\n",
       "       [0.5710443 ],\n",
       "       [0.58340365],\n",
       "       [0.59924054],\n",
       "       [0.4915202 ],\n",
       "       [0.5429754 ],\n",
       "       [0.5321423 ],\n",
       "       [0.5340592 ],\n",
       "       [0.58086586],\n",
       "       [0.5176376 ],\n",
       "       [0.48329934],\n",
       "       [0.5265461 ],\n",
       "       [0.5434443 ],\n",
       "       [0.57995546],\n",
       "       [0.45440915],\n",
       "       [0.51469356],\n",
       "       [0.46747175],\n",
       "       [0.4798045 ],\n",
       "       [0.54771906],\n",
       "       [0.5483517 ],\n",
       "       [0.5852723 ],\n",
       "       [0.60030913],\n",
       "       [0.49232832],\n",
       "       [0.5121715 ],\n",
       "       [0.5246455 ],\n",
       "       [0.5017493 ],\n",
       "       [0.5646295 ],\n",
       "       [0.4844697 ],\n",
       "       [0.49964896],\n",
       "       [0.5539731 ],\n",
       "       [0.5280769 ],\n",
       "       [0.5296947 ],\n",
       "       [0.6358349 ],\n",
       "       [0.5494626 ],\n",
       "       [0.47350743],\n",
       "       [0.44118297],\n",
       "       [0.4042386 ],\n",
       "       [0.4089081 ],\n",
       "       [0.595505  ],\n",
       "       [0.5057993 ],\n",
       "       [0.6143968 ],\n",
       "       [0.55171424],\n",
       "       [0.5871034 ],\n",
       "       [0.41795677],\n",
       "       [0.4310805 ],\n",
       "       [0.5873242 ],\n",
       "       [0.5591289 ],\n",
       "       [0.5125399 ],\n",
       "       [0.4765651 ],\n",
       "       [0.4637421 ],\n",
       "       [0.51989174],\n",
       "       [0.5092983 ],\n",
       "       [0.5076269 ],\n",
       "       [0.49689957],\n",
       "       [0.44151804],\n",
       "       [0.46996248],\n",
       "       [0.5807039 ],\n",
       "       [0.5211468 ],\n",
       "       [0.5555485 ],\n",
       "       [0.5279864 ],\n",
       "       [0.5395702 ],\n",
       "       [0.5218049 ],\n",
       "       [0.6387617 ],\n",
       "       [0.5353662 ],\n",
       "       [0.58987993],\n",
       "       [0.5446512 ],\n",
       "       [0.5778339 ],\n",
       "       [0.5075774 ],\n",
       "       [0.5575303 ],\n",
       "       [0.49944678],\n",
       "       [0.6568996 ],\n",
       "       [0.4741121 ],\n",
       "       [0.4644196 ],\n",
       "       [0.43373057],\n",
       "       [0.5730153 ],\n",
       "       [0.579244  ],\n",
       "       [0.5376222 ],\n",
       "       [0.5816078 ],\n",
       "       [0.525045  ],\n",
       "       [0.46527216],\n",
       "       [0.5261661 ],\n",
       "       [0.4448437 ],\n",
       "       [0.4808344 ],\n",
       "       [0.62273085],\n",
       "       [0.5015591 ],\n",
       "       [0.6155071 ],\n",
       "       [0.67318547],\n",
       "       [0.70883465],\n",
       "       [0.51796764],\n",
       "       [0.6010208 ],\n",
       "       [0.4536706 ],\n",
       "       [0.54753476],\n",
       "       [0.51547474],\n",
       "       [0.5055228 ],\n",
       "       [0.5084932 ],\n",
       "       [0.6421423 ],\n",
       "       [0.4879325 ],\n",
       "       [0.606037  ],\n",
       "       [0.5354152 ],\n",
       "       [0.60421354],\n",
       "       [0.4520248 ],\n",
       "       [0.502346  ],\n",
       "       [0.6191002 ],\n",
       "       [0.47537738],\n",
       "       [0.5716187 ],\n",
       "       [0.46267104],\n",
       "       [0.5809953 ],\n",
       "       [0.52593845],\n",
       "       [0.6248851 ],\n",
       "       [0.49872297],\n",
       "       [0.5517892 ],\n",
       "       [0.6451766 ],\n",
       "       [0.46069402],\n",
       "       [0.56356126],\n",
       "       [0.4973791 ],\n",
       "       [0.35377616],\n",
       "       [0.65917563],\n",
       "       [0.53691524],\n",
       "       [0.6623652 ],\n",
       "       [0.5410266 ],\n",
       "       [0.55209434],\n",
       "       [0.6510383 ],\n",
       "       [0.55845654],\n",
       "       [0.5806242 ],\n",
       "       [0.5927398 ],\n",
       "       [0.6234597 ],\n",
       "       [0.49145254],\n",
       "       [0.59908473],\n",
       "       [0.49168155],\n",
       "       [0.6115434 ],\n",
       "       [0.46608236],\n",
       "       [0.5416068 ],\n",
       "       [0.5285821 ],\n",
       "       [0.58596843],\n",
       "       [0.54950273],\n",
       "       [0.5635744 ],\n",
       "       [0.535547  ],\n",
       "       [0.5241493 ],\n",
       "       [0.522915  ],\n",
       "       [0.5009923 ],\n",
       "       [0.51909447],\n",
       "       [0.43996218],\n",
       "       [0.49176762],\n",
       "       [0.5312508 ],\n",
       "       [0.53256935],\n",
       "       [0.52161354],\n",
       "       [0.4890744 ],\n",
       "       [0.5698382 ],\n",
       "       [0.56181055],\n",
       "       [0.47402617],\n",
       "       [0.52340597],\n",
       "       [0.47045606],\n",
       "       [0.5468891 ],\n",
       "       [0.47588667],\n",
       "       [0.54773176],\n",
       "       [0.5895464 ],\n",
       "       [0.51412666],\n",
       "       [0.58095336],\n",
       "       [0.5953051 ],\n",
       "       [0.5110231 ],\n",
       "       [0.54981667],\n",
       "       [0.5092184 ],\n",
       "       [0.5239954 ],\n",
       "       [0.5929474 ],\n",
       "       [0.55197525],\n",
       "       [0.5181829 ],\n",
       "       [0.5113926 ],\n",
       "       [0.53121704],\n",
       "       [0.4739324 ],\n",
       "       [0.51801026],\n",
       "       [0.5294143 ],\n",
       "       [0.5243701 ],\n",
       "       [0.61109126],\n",
       "       [0.51620907],\n",
       "       [0.5650225 ],\n",
       "       [0.46491274],\n",
       "       [0.5011474 ],\n",
       "       [0.53302014],\n",
       "       [0.51112396],\n",
       "       [0.5985587 ],\n",
       "       [0.56975543],\n",
       "       [0.47190642],\n",
       "       [0.45356748],\n",
       "       [0.5843006 ],\n",
       "       [0.5071279 ],\n",
       "       [0.60662377],\n",
       "       [0.51431054],\n",
       "       [0.6910016 ],\n",
       "       [0.5878929 ],\n",
       "       [0.60999155],\n",
       "       [0.47708532],\n",
       "       [0.5895671 ],\n",
       "       [0.6148523 ],\n",
       "       [0.5470098 ],\n",
       "       [0.5832223 ],\n",
       "       [0.4720833 ],\n",
       "       [0.5975123 ],\n",
       "       [0.45697007],\n",
       "       [0.42744276],\n",
       "       [0.5918934 ],\n",
       "       [0.58575267],\n",
       "       [0.56779367],\n",
       "       [0.44680154],\n",
       "       [0.5098    ],\n",
       "       [0.60305566],\n",
       "       [0.5835604 ],\n",
       "       [0.55349576],\n",
       "       [0.5580473 ],\n",
       "       [0.46859205],\n",
       "       [0.61443084],\n",
       "       [0.5505495 ],\n",
       "       [0.4991864 ],\n",
       "       [0.4578976 ],\n",
       "       [0.6157431 ],\n",
       "       [0.70493495],\n",
       "       [0.43956092],\n",
       "       [0.5782771 ],\n",
       "       [0.49954847],\n",
       "       [0.5993042 ],\n",
       "       [0.535066  ],\n",
       "       [0.47370914],\n",
       "       [0.5786347 ],\n",
       "       [0.5611995 ],\n",
       "       [0.5428848 ],\n",
       "       [0.44056618],\n",
       "       [0.52862495],\n",
       "       [0.5210259 ],\n",
       "       [0.528264  ],\n",
       "       [0.6157591 ],\n",
       "       [0.4644041 ],\n",
       "       [0.51831824],\n",
       "       [0.58534896],\n",
       "       [0.39274758],\n",
       "       [0.5451313 ],\n",
       "       [0.5630662 ],\n",
       "       [0.5760573 ],\n",
       "       [0.61181486],\n",
       "       [0.57242584],\n",
       "       [0.48978624],\n",
       "       [0.53362733],\n",
       "       [0.56235427],\n",
       "       [0.5367067 ],\n",
       "       [0.63123447],\n",
       "       [0.5161089 ],\n",
       "       [0.5535976 ],\n",
       "       [0.6178333 ],\n",
       "       [0.4862375 ],\n",
       "       [0.56897545],\n",
       "       [0.48723692],\n",
       "       [0.6336695 ],\n",
       "       [0.5067313 ],\n",
       "       [0.5163639 ],\n",
       "       [0.4874879 ],\n",
       "       [0.494709  ],\n",
       "       [0.49238116],\n",
       "       [0.5954396 ],\n",
       "       [0.65389574],\n",
       "       [0.52142763],\n",
       "       [0.5824534 ],\n",
       "       [0.49554893],\n",
       "       [0.586097  ],\n",
       "       [0.53590846],\n",
       "       [0.49260154],\n",
       "       [0.60668623],\n",
       "       [0.50820273],\n",
       "       [0.5436338 ],\n",
       "       [0.56599724],\n",
       "       [0.4955972 ],\n",
       "       [0.58242327],\n",
       "       [0.5058291 ],\n",
       "       [0.6253167 ],\n",
       "       [0.5437041 ],\n",
       "       [0.42282766],\n",
       "       [0.47833082],\n",
       "       [0.6299679 ],\n",
       "       [0.6557714 ],\n",
       "       [0.42474028],\n",
       "       [0.63450265],\n",
       "       [0.5921606 ],\n",
       "       [0.4796418 ],\n",
       "       [0.51710176],\n",
       "       [0.4791194 ],\n",
       "       [0.3873628 ],\n",
       "       [0.55507916],\n",
       "       [0.48148265],\n",
       "       [0.4530567 ],\n",
       "       [0.5119491 ],\n",
       "       [0.5952159 ],\n",
       "       [0.6301497 ],\n",
       "       [0.51324207],\n",
       "       [0.62386894],\n",
       "       [0.6294781 ],\n",
       "       [0.54643935],\n",
       "       [0.6373199 ],\n",
       "       [0.43753204],\n",
       "       [0.5064964 ],\n",
       "       [0.4576676 ],\n",
       "       [0.39543408],\n",
       "       [0.5359108 ],\n",
       "       [0.59900963],\n",
       "       [0.52454275],\n",
       "       [0.44269034],\n",
       "       [0.5092987 ],\n",
       "       [0.5751498 ],\n",
       "       [0.57598484],\n",
       "       [0.5564311 ],\n",
       "       [0.44365227],\n",
       "       [0.5758392 ],\n",
       "       [0.5522732 ],\n",
       "       [0.5287346 ],\n",
       "       [0.5862236 ],\n",
       "       [0.61913943],\n",
       "       [0.49446073],\n",
       "       [0.50724906],\n",
       "       [0.57449627],\n",
       "       [0.57661146],\n",
       "       [0.6451338 ],\n",
       "       [0.6529244 ],\n",
       "       [0.64387655],\n",
       "       [0.57414365],\n",
       "       [0.51640326],\n",
       "       [0.57317156],\n",
       "       [0.50558686],\n",
       "       [0.53334546],\n",
       "       [0.4571467 ],\n",
       "       [0.53343946],\n",
       "       [0.47204658],\n",
       "       [0.44472829],\n",
       "       [0.57409644],\n",
       "       [0.5380021 ],\n",
       "       [0.52514434],\n",
       "       [0.50623804],\n",
       "       [0.57499313],\n",
       "       [0.5948829 ],\n",
       "       [0.47079757],\n",
       "       [0.5410575 ],\n",
       "       [0.55049187],\n",
       "       [0.61353415],\n",
       "       [0.54181564],\n",
       "       [0.48442242],\n",
       "       [0.5594523 ],\n",
       "       [0.6715901 ],\n",
       "       [0.4731269 ],\n",
       "       [0.6228464 ],\n",
       "       [0.5218674 ],\n",
       "       [0.56970495],\n",
       "       [0.578103  ],\n",
       "       [0.5837587 ],\n",
       "       [0.5313679 ],\n",
       "       [0.5168598 ],\n",
       "       [0.61218673],\n",
       "       [0.4043343 ],\n",
       "       [0.4414801 ],\n",
       "       [0.49897647],\n",
       "       [0.49822605],\n",
       "       [0.49480423],\n",
       "       [0.538377  ],\n",
       "       [0.49550807],\n",
       "       [0.5591653 ],\n",
       "       [0.558627  ],\n",
       "       [0.5113084 ],\n",
       "       [0.43448856],\n",
       "       [0.59963244],\n",
       "       [0.5210182 ],\n",
       "       [0.533172  ],\n",
       "       [0.56277204],\n",
       "       [0.46209013],\n",
       "       [0.4710573 ],\n",
       "       [0.38567305],\n",
       "       [0.51867205],\n",
       "       [0.49150616],\n",
       "       [0.59228665],\n",
       "       [0.5062874 ],\n",
       "       [0.50683314],\n",
       "       [0.5984997 ],\n",
       "       [0.47163212],\n",
       "       [0.627573  ],\n",
       "       [0.5060715 ],\n",
       "       [0.57554126],\n",
       "       [0.5570886 ],\n",
       "       [0.63501424],\n",
       "       [0.55859256],\n",
       "       [0.5182954 ],\n",
       "       [0.61613214],\n",
       "       [0.57871556],\n",
       "       [0.65181947],\n",
       "       [0.5130813 ],\n",
       "       [0.51666534],\n",
       "       [0.5755763 ],\n",
       "       [0.46981353],\n",
       "       [0.4794296 ],\n",
       "       [0.61046386],\n",
       "       [0.50077134],\n",
       "       [0.5260161 ],\n",
       "       [0.509824  ],\n",
       "       [0.5870474 ],\n",
       "       [0.46008503],\n",
       "       [0.4199245 ],\n",
       "       [0.5363921 ],\n",
       "       [0.59281635],\n",
       "       [0.575356  ],\n",
       "       [0.56464124],\n",
       "       [0.5452522 ],\n",
       "       [0.51201886],\n",
       "       [0.60757613],\n",
       "       [0.5942029 ],\n",
       "       [0.45369   ],\n",
       "       [0.52265155],\n",
       "       [0.56694514],\n",
       "       [0.5213943 ],\n",
       "       [0.4850337 ],\n",
       "       [0.5676468 ],\n",
       "       [0.5268705 ],\n",
       "       [0.59407943],\n",
       "       [0.6117222 ],\n",
       "       [0.5608104 ],\n",
       "       [0.5534297 ],\n",
       "       [0.46101907],\n",
       "       [0.4982161 ],\n",
       "       [0.50742483],\n",
       "       [0.5004167 ],\n",
       "       [0.6143202 ],\n",
       "       [0.42979562],\n",
       "       [0.62508357],\n",
       "       [0.5539253 ],\n",
       "       [0.50585234],\n",
       "       [0.40970463],\n",
       "       [0.54990333],\n",
       "       [0.563866  ],\n",
       "       [0.50632375],\n",
       "       [0.51598644],\n",
       "       [0.5210067 ],\n",
       "       [0.59085673],\n",
       "       [0.543144  ],\n",
       "       [0.57504916],\n",
       "       [0.5370952 ],\n",
       "       [0.5525577 ],\n",
       "       [0.53333455],\n",
       "       [0.504937  ],\n",
       "       [0.51820517],\n",
       "       [0.48223603],\n",
       "       [0.54376775],\n",
       "       [0.5863325 ],\n",
       "       [0.5997209 ],\n",
       "       [0.5945756 ],\n",
       "       [0.55737805],\n",
       "       [0.52227414],\n",
       "       [0.5119497 ],\n",
       "       [0.57944894],\n",
       "       [0.61345744],\n",
       "       [0.5417664 ],\n",
       "       [0.49240512],\n",
       "       [0.5532928 ],\n",
       "       [0.50778437],\n",
       "       [0.5198715 ],\n",
       "       [0.426161  ],\n",
       "       [0.5627617 ],\n",
       "       [0.6082178 ],\n",
       "       [0.61154336],\n",
       "       [0.57350147],\n",
       "       [0.5932696 ],\n",
       "       [0.58516574],\n",
       "       [0.6917117 ],\n",
       "       [0.5734081 ],\n",
       "       [0.58616096],\n",
       "       [0.46247897],\n",
       "       [0.5868168 ],\n",
       "       [0.48684153],\n",
       "       [0.5885721 ],\n",
       "       [0.46887618],\n",
       "       [0.545781  ],\n",
       "       [0.4723081 ],\n",
       "       [0.5025746 ],\n",
       "       [0.5798459 ],\n",
       "       [0.47238454]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(\n",
    "            data,\n",
    "            batch_size=32,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
